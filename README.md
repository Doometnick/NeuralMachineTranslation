# NeuralMachineTranslation
Translating languages with a transformer architecture

This is a simple example of using a transformer architecture to perform translations from German to English. The key is that no recurrent neural networks such as LSTMs or GRUs are employed, which significantly increases the training process.

The code and architecture are taken from Google's paper [Attention is All You Need](https://arxiv.org/pdf/1706.03762.pdf) and Google's [GitHub repository](https://github.com/tensorflow/docs/blob/master/site/en/tutorials/text/transformer.ipynb) with minor adjustments and different datasets.
